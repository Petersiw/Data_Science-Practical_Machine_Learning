---
title: "Practical Machine Learning: Course Project"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    keep_md: yes
    theme: spacelab
author: "Ming Wei Siw"
---

##Summary

This study is based on Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements by Ugulino et al.. Using the data acquired from their paper, this study attempts to predict the various body movements of the subjects based on accelerometers data from different parts of the body.

To cite the paper, 
"Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

To classify correctly the actions of the various subjects, various prediction methods have been used, which are, random forest, boosting and model stacking. The most accurate method appears to be random forest, albeit requiring length computation time.

##Load Packages

```{r, message = F}
library(data.table)
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
```

##Download and Load Data

```{r}
urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#download training data
if(!file.exists("pml-training.csv")) 
  {download.file(urltrain, destfile = "pml-training.csv", method = "curl")}
#load data
data1 <- fread("pml-training.csv", header = T)
```

##Data Cleansing

Since the data also contains values not suitable for prediction, i.e., subjects names, they are removed from the training dataset. Also, there are variables in the data which contains too many empty observations to be useful for analysis, i.e., kurtosis, amplitude and skewness. To ease prediction, these are also removed from the dataset.

```{r}
#isolate variables of interest
data1.predictors <- data1[, -c(1:7)]
data1.predictors$classe <- as.factor(data1.predictors$classe)
#removing variables with NAs
na.sum.predictors <- apply(is.na(data1.predictors), 2, sum)
data1.predictors <- subset(data1.predictors, 
                              select = which(na.sum.predictors == 0))
#remove predictors with many empty entries
rem.kurt <- grepl("^kurtosis", names(data1.predictors))
data1.predictors <- data1.predictors[, !rem.kurt, with = F]
rem.skew <- grepl("^skewness", names(data1.predictors))
data1.predictors <- data1.predictors[, !rem.skew, with = F]
rem.max <- grepl("^max", names(data1.predictors))
data1.predictors <- data1.predictors[, !rem.max, with = F]
rem.min <- grepl("^min", names(data1.predictors))
data1.predictors <- data1.predictors[, !rem.min, with = F]
rem.amp <- grepl("^amplitude", names(data1.predictors))
data1.predictors <- data1.predictors[, !rem.amp, with = F]
```

##Split Data into Training, Test and Validation Set

The given dataset is split into three portions: training, testing and validation.

```{r}
set.seed(1007)
inBuild <- createDataPartition(data1.predictors$classe, p = 0.7, list = F)
validation <- data1.predictors[-inBuild, ]
build <- data1.predictors[inBuild, ]
inTrain <- createDataPartition(build$classe, p = 0.7, list = F)
training <- build[inTrain, ]
testing <- build[-inTrain, ]
```

##Configure Parallel Processing

To hasten calculations, parallel processing is used in this study.

```{r, message = F}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

##Modelling

###Cross Validation

To enhance the prediction powers of these models, cross validation with 5 folds are repeated for 5 times.

```{r}
control <- trainControl(method = "repeatedcv", number = 5, repeats = 5,
                        allowParallel = T)
```

All the prediction methods used in this study are ensemble methods as they usually result in the highest accuracy while reducing both bias and variance.

###Random Forest

Random forest attemps to build many classification trees using subsets of both data and predictors. This form of resampling allows for low bias and variance, while preventing the problem of overfitting.

```{r, warning = F, message = F, cache=T}
m1 <- train(classe ~ ., method = "rf", data = training, trControl = control)
confusionMatrix(testing$classe, predict(m1, testing))
```

###Boosting

Boosting combines weak predictors to form a strong predictor. In the process, weaker predictors are weighted and then combined to form the final predictor.

```{r, warning = F, message = F, cache = T}
m2 <- train(classe ~., method = "gbm", data = training, verbose = F,
            trControl = control)
confusionMatrix(testing$classe, predict(m2, testing))
```

###Bagging

Bagging, or Bootstrap Aggregating, creates bootstrap samples from the training set and fits a model using each resample. By averaging the fits from each samples, bagging also reduces bias and variance while avoiding overfitting.

```{r, warning = F, message = F, cache = T}
m3 <- train(classe ~., method = "treebag", data = training, 
            trControl = control)
confusionMatrix(testing$classe, predict(m3, testing))
```

###Model Stacking With Validation Data

The three methods above are used together to reduce bias and variance. Moreover, model stacking also reduces the chance of overfitting. The validation dataset are used here to check for the accuracy of the prediction algorithm.

```{r, warning = F, message = F, cache = T}
stack.df <- data.frame(rf = predict(m1, testing), gbm = predict(m2, testing), 
                       bag = predict(m3, testing), classe = testing$classe)
comb.fit1 <- train(classe ~., data = stack.df, method = "rf", 
                   trControl = control)
pred.va.rf <- predict(m1, validation)
pred.va.boost <- predict(m2, validation)
pred.va.bag <- predict(m3, validation)
stack.df.va <- data.frame(rf = pred.va.rf, gbm = pred.va.boost,
                          bag = pred.va.bag, classe = validation$classe)
confusionMatrix(stack.df.va$classe, predict(comb.fit1, stack.df.va))
```

As can be seen from prediction with validation data, the accuracy of the stacked model is 99%. Therefore the expected out-of-sample error is around 1%.

##Prediction With Testing Dataset

###Download and Load Data

```{r}
urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download testing data
if(!file.exists("pml-testing.csv")) 
  {download.file(urltest, destfile = "pml-testing.csv", method = "curl")}
#load data
data2 <- fread("pml-testing.csv", header = T)
```

###Prediction with Stacked Model

```{r, message = F, cache = T}
stack.pred <- data.frame(rf = predict(m1, data2), gbm = predict(m2, data2),
                         bag = predict(m3, data2))
pred <- predict(comb.fit1, stack.pred)
```

The predicted classes of activities are `r pred`.

##De-Register Parallel Processing

Deregistering parallel processing will end the use of multithreading for R.

```{r}
stopCluster(cluster)
registerDoSEQ()
```

##Conclusion

In short, all the models works very well in predicting the different types of movements made by the subjects, each with an accuracy greater than 95%. Prediction with validation data using the stacked model showed that out-of-sample error rate is around 1%.